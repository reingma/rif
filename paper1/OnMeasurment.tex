\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb, physics, geometry, mdframed, authblk}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{mathrsfs}
\geometry{margin=1in}

\title{On Measurement: The Relativity of Information Frames}
\author[ ]{Gabriel Masarin}
\affil[ ]{\texttt{g.masarin@proton.me}}
\date{19 November, 2025}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\usepackage{setspace}
\setstretch{1.07}
\clubpenalty=10000
\widowpenalty=10000
\setlength{\belowdisplayskip}{0.8\baselineskip}
\setlength{\abovedisplayskip}{0.8\baselineskip}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newmdenv[
  backgroundcolor=gray!10,
  linecolor=gray!50,
  linewidth=1pt,
  roundcorner=6pt,
  innertopmargin=8pt,
  innerbottommargin=8pt,
]{principlebox}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheoremstyle{axiomstyle}% <name>
  {8pt}% <Space above>
  {8pt}% <Space below>
  {\itshape}% <Body font>
  {}% <Indent amount>
  {\bfseries}% <Theorem head font>
  {.}% <Punctuation after theorem head>
  {0.5em}% <Space after theorem head>
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}% <Theorem head spec>

\theoremstyle{axiomstyle}
\newtheorem{axiom}{Axiom}
\begin{document}
\maketitle

\vspace{0.5em}
\begin{abstract}

\end{abstract}

\section{Introduction}
In this paper we begin with a motivational intuition. That of a principle of relativity.
We then throught the paper endevor to make that intuition mathematically precise. First
by introducing the tools we will use, sheaf theory contextuality and markov kernels. Then
we use these tools to give a precise meaning to this relativity principle.

Finally we show that this principle generates a space that has the properties of a Hilbert
space. Using Gleanson's theorem \cite{gleanson} we retrieve the born rule. We give a natural
explanation for the pointer basis and give as a theorem Wigner friend's consistency,
natural Markoviality of physical systems and the arrow of time.

We conclude with a ontological intrepetation of this principle and suggestions for further
reasearch directions. We conclude that, if this principle is accepted as a valid restriction
for reality, the Copehangen intrepetation axioms are derived instead of postulated. Giving
a potential solution to the measurement problem.

We also compare it to other explanations such as GRW and Penrose collapse. Also relates
to its direct cousin, Relation Quantum Mechanics. This is not a intrepretation but a new
framework.

Acceptance of this principle depends upon accepting the Relativity of information frames
as physically fundamental. Further work is needed to see its consequences, in principle
it does not disagree with quantum mechanics and provides a clean resolution to some of
its puzzling features. No experiment to derive prove its truth is known to the authors of
this paper at the current formulation.

\section{Motivation - The Relativity of Information Frames}

Consider two physical observers, Alice and Bob, each equipped with a clock and a ruler.
To infer a particle’s momentum, they make two position measurements and record 
the elapsed time.

However,
\begin{itemize}
    \item if they agree on the spatial separation, they must disagree on the elapsed time;
    \item if they agree on the elapsed time, the measured spatial separation must differ.
\end{itemize}

Their interactions with the world differ — and so does what each can resolve as an event.

What Alice calls “particle at position $x$ at time $t$” is determined by her 
interaction channels and detection thresholds.

Thus there is no global, frame-independent $\sigma$-algebra of events.  
Every physical system carries its own information frame: a $\sigma$-algebra 
of distinguishable outcomes accessible through its interactions.

Einstein taught that coordinate descriptions are relative while causal order is invariant.  
We extend this principle.

\begin{principlebox}
\begin{center}
\textbf{Relativity of Information Frames (RIF)}

Nature does not favor one perspective over another. Nature is the same no matter the
frame of information.
\end{center}

\end{principlebox}

Measurement is not the revelation of a pre-existing global state; it is the joint refinement
(and, when necessary, coarse-graining) of information frames when systems interact.  
From this symmetry, quantum state update, pointer bases, and even causal 
geometry follow as consequences.

\section{Background}
\subsection{Measure Theory}
The complete introduction to the richness of measure theory probability theory 
is not in the scope of this work, we refer to \cite{Williams} for that, 
we will at least the concept of probability space. We hope the work is understandable with
only this crude introduction but familiarity with the subject is advised.


\subsubsection*{$\sigma$-algebras}
\subsubsection*{Probability Measure}
\subsubsection*{Probability Spaces}
Throughout this paper, we will often not be using specific probability measures, 
working only with the sample space and the $\sigma$-algebras.
\subsubsection*{Measurable Functions}
\subsubsection*{Pushfoward Measure}
\subsubsection*{Markov Kernels}
We will also need the definition of \emph{Markov Kernels}, which give us to talk about 
how different probability spaces interact.
\begin{definition}[Markov Kernels]
    Let $(\Omega_1, \mathcal F_1)$ and $(\Omega_2, \mathcal F_2)$ be two measurable 
    spaces. A \emph{Markov Kernel} is a function:
    \[
        K: \Omega_1 \cross \mathcal F_2 \to [0,1]
    \]
    \noindent
    Where we have:
    \begin{itemize}
        \item For every fixed $\omega \in \Omega_1$ 
            \[
                K(\omega,\cdot) \text{ is a probability measure in } \mathcal F_2
            \]
        \item For every fixed $A \in \mathcal F_2$
            \[
                K(\cdot, A) \text{ is a measurable function } \Omega_1 \to
                \left([0,1],\mathcal B\left([0,1]\right)\right)
            \]
    \end{itemize}
    \noindent
    The idea is that Markov Kernels define probability measures on the target space that
    respect the structure of the source space. It is often written $K: \Omega_1 \to 
    \mathcal P(\Omega_2)$ to say, a Kernel that defines probabilities on $\Omega_2$ from
    $\Omega_1$.
\end{definition}
Markov Kernels allow us to define probability measures on the target space from measures
on the source.
\begin{definition}[Markov Pushfoward Measure]
    Given the measurable spaces and Kernel on the Definition 3~4. Let $\mu$ be a probability 
    measure on $\Omega_1$. The \emph{pushfoward measure} given by the Kernel
    \[
        (\mu K) (A) := \int_{\Omega_1} K(\omega, A) \,\mu (dx), \qquad A \in \mathcal F_2
    \]
    \noindent
    And it is a probability measure on $\mathcal F_2$.
\end{definition}

And finally, we also need to look at the definition of a 
\emph{effective $\sigma$-algebra} of a Markov Kernel.
\begin{definition}[Effective $\sigma$-algebra of K]
    For a Kernel $K: \left(\Omega_1,\mathcal F_1\right) \to
    \left(\Omega_2,\mathcal F_2\right)$. The effective sigma algebra of 
    $K$ is given by:
    \[
        \mathcal F_K := \sigma\left\{\omega \to K(\omega,A) 
        \mid A \in \mathcal F\right\}
    \]
    \noindent
    That is, the algebra of all random variables that $K$ generates. It can also be seen
    as the algebra of events of $\Omega_1$ that remain distinguishable after passing 
    through $K$.
\end{definition}
\subsubsection*{Composition of Markov Kernels}
Markov Kernels compose, in particular if $K_1: \Omega_1 \to \mathcal P(\Omega_2)$ and
$K_2: \Omega_2 \to \mathcal P(\Omega_3)$ are Markov Kernels. Then their composite is
given for $A \in \mathcal F_3$:
\[
    \left(K_2 \circ K_1\right)(x,A) := \int_{\Omega_2} K_2(y,A) d K_1(x,\cdot)
\]
And naturally defines probability measures on $\Omega_3$.
\subsubsection*{Deterministic Markov Kernels}
These are a special class of Markov Kernels that can act as transport structure from one 
probability space to another.
\begin{definition}[Deterministic Markov Kernels]
    Let $(\Omega_1, \mathcal F_1)$ and $(\Omega_2, \mathcal F_2)$ be two measurable 
    spaces. A \emph{Deterministic Markov Kernel} is induced by a measurable function
    $f: \Omega_1 \to \Omega_2$:
    \[
        K_f: \Omega_1 \cross \mathcal F_2 \to [0,1]
    \]
    \noindent
    That is given for $\omega_1 \in \Omega_1$ and $F_2 \in \mathcal F_2$
    \[
        K_f(\omega_1,F_2) := \bold{1}_{f(\omega_1) \in F_2}
    \]

\end{definition}
The function $f$ can be seen as the transport from one probability space into another. The
Kernel then, allows us to pushfoward probability measures from it.
\subsubsection*{Embeddings}
As we have seen, we can use a measurable function $f: \Omega_1 \to \Omega_2$ to define
a Markov Kernel. The Markov Kernel then allows us to transport probabilities to the new
space.

There are a few special measurable functions we will be interested in. The first defines
a full \emph{isomorphism} of spaces:
\begin{definition}[Measurable Isomorphism]
    A measurable function $T: \Omega_1 \to \Omega_2$ that is \emph{bijective} and whose
    inverse $T^{-1}: \Omega_2 \to \Omega_1$ is also measurable defines a \emph{isomorphism}
    of probability spaces:
    \[
        \left(\Omega_1, \mathcal F_1\right) \cong \bigl(\Omega_2, \mathcal F_2\bigr)
    \]
\end{definition}
Naturally such bijections form a group:
\begin{definition}[Measurable Space Automorphisms]
\[
    \text{Aut}_{\Omega,\mathcal F} := \left\{T: \Omega \to \Omega 
    \text{ is bijective, } T\text{ and }T^{-1}\text{ are measurable }\right\}
\]
\end{definition}
These maps preserve the full structure of the measurable space, for our work we will need
a class that still preserves structure but can embed the measurable space into a larger
one.
\begin{definition}[Measurable Embedding]
    A measurable embedding $\iota: \Omega_1 \to \Omega_2$ is a \emph{injective} measurable
    function with a measurable inverse. It can also be seen as a local homomorphism.
\end{definition}
These embeddings preserve the $\sigma$-algebra of the source space entirely in the target
space.
\subsection{Contextuality}
The first important concept the theory relies upon is that of contextuality. All our
definitions here are translated from the \cite{Abramsky&Brandenburger} contextuality
in sheaf-theory. They have been adapted to a measure theory framework.
\subsubsection*{Labels and Contexts}
First we look at the definition of a measurement label. A measurement label intuitively
represents what one can tell apart, that is what questions a system can ask. It can be
seen as the fundamental degrees of freedom of a given model. 
\subsubsection*{Measurement Labels}
\begin{definition}[Measurement labels]
    A measurement label is an abstract symbol $m$ that identifies a physical distinction
    we may attempt to extract from the system. Together with its outcome 
    space $(\Omega_m, \mathcal F_m)$. That is:
    \[
        m \to (\Omega_m, \mathcal F_m)
    \]
    The set of all measurement labels the model considers primitive is called $\mathcal M$.
\end{definition}

Then naturally, our global space, where all measurament labels exist is then:
\begin{definition}[Global Space]
    The global space, the space of all degrees of freedom and all their distinctions is 
    \[
        \left(\Omega_{\mathcal M}, \mathcal F_{\mathcal M}\right) = 
        \left(\prod_{m \in \mathcal M} \Omega_m \;, \bigotimes_{m \in \mathcal M} 
        \mathcal F_m \right)
    \]
    \noindent
    Where $\mathcal F_1 \otimes \mathcal F_2 = 
    \sigma\left(\left\{F_1 \times F_2: F_1 \in \mathcal F_1, F_2 \in 
    \mathcal F_2\right\}\right)$. We note that, we do not define a particular probability measure on this space, that
    is because what we are interested in at the moment is the structure of the space,
    not a specfic measure on it.
\end{definition}

\subsubsection*{Contexts}
Next we talk about contexts. Contexts are given by the subset of labels or degrees of
freedom a given observer cares about something he is interacting with. It can be seen
as the fundamental set of questions he can ask about the part of the model he interacts 
with.

\begin{definition}[Context]
    A context $C \subseteq \mathcal M$ is a finite collection of 
    measurement labels that are jointly meaningful. 
    To each context we associate a measurable space:
    \[
        (\Omega_{C}, \mathcal F_{C}) := \left(\prod_{m \in C}
        \Omega_m \;, \bigotimes_{m \in C} \mathcal F_m \right)
    \]
\end{definition}

Within a context we also define the projections:
\begin{definition}[Canonical Context Projections]
    The projections for a context $C$ are defined as \emph{measurable functions} from a
    context to one of its label spaces.
\[
    \pi_{C \to \{m\}}: \Omega_C \to \Omega_m
\]
With:
\[
    \pi_{C \to \{m\}}\left(\omega\right) = \omega_m \quad \forall \omega \in C
\]
\noindent
That is, the projections map to the context corresponding to the label $m$ within
the context $C$. The extension to a subcontext $D \subseteq C$ is naturally
$\pi_{C\to D}$.
\end{definition}
Intuitively projection can be seen as the \emph{perspective} $C$ has on $m$. 
For context projection we will also need its inverse definition.
\begin{definition}[Cylinder Embedding]
    For every outcome $\omega_1 \in \Omega_m$ we embed it in the context space 
    $(\Omega_C,\mathcal F_C)$. This gives the definition:
    \[
        \pi_{C\to\{m\}}^{-1}(A) := \left\{\omega \in \Omega_C:\; x_m \in A\right\}
    \]
    \noindent
    Since $\pi$ is a measurable function by definition $\pi_{C\to\{m\}}^{-1}(A) \in 
    \mathcal F_C$.
\end{definition}

We importantly note that, a context require $\pi$. They tell the context where its events
come from. In principle, a context does not hold \emph{information that comes from nowhere.}

\subsubsection*{Empirical Model}
We will now introduce the first concept that requires the use of specific probability 
measures that is the definition of an \emph{empirical model}. 

Intuitively can be seen as a particular realization of the model, or a 
particular realization of a \emph{perspective} on the underlying world. 
We can also think of it as a particular family of \emph{coodinates} in the probability 
spaces of the contexts.
\begin{definition}[Empirical model]
    An \emph{empirical model} is a family $\left\{e_C \right\}_{ C \in \mathcal M}$ 
    of probability measures on $\left(\Omega_C, \mathcal F_C\right)$. 
    For all $C,C' \in \mathcal M$ and all $D \in C \cap C'$ we have:
    \[
        \left(\pi_{C \to D}\right)_* e_C \; = \; \left(\pi_{C' \to D}\right)_* e_{C'}
    \]
    This condition means that on overlaps, the probability measures must agree. They
    come from the same underlying labels.
\end{definition}

\subsubsection*{Contextuality}
The empirical families allows us to define what will be the driving feature of our 
framework. It is the defintion of \emph{Contextuality}. When \emph{perspectives} 
only completly exist on the context they came from.

\begin{definition}[Contextuality]
    The family $\left\{e_{E} \right\}_{ E \in \mathcal M}$ is called contextual 
    in $\mathcal M$ if no probability measure $\mu$ on the global space $\mathcal M$ 
    exists satisfying:
    \[
        \left(\pi_{C \to E}\right)_* \mu \; = e_{E} \quad \forall E \subset \mathcal M
    \]
    \noindent
    They are called non-contextual, if such probability measure exists.
\end{definition}

Intuitively, it means that in that shared space, the questions still make perfect sense
together if they are non-contextual. We know exactly where they came from.

If they are contextual then there is no way to pick a coordinate, or probability measure
on the global space that agrees with all probabilities the contexts of that space found.

The core feature we will need here is that, there exists experiments or real situations
where the global space is contextual this fact can be seen in depth in 
\cite{Abramsky&Brandenburger}. This particular definition, is of probabilistic 
contextuality as defined on \cite{Abramsky&Brandenburger}.
\section{Structural Contextuality}
To get a better understanding on RIF we need to look at other formulations of 
contextuality. We want to understand contextuality without particular probability 
distributions.


For this, we adapt the sheaf like condition, than just the failure of probabilities to
have the correct marginals, this in particular matches the \emph{strong contextuality} 
defined in \cite{Abramsky&Brandenburger}. First we define a \emph{Information Frame}.

\begin{definition}[Information Frame]
    A information frame is a probability space over a context $C \subseteq \mathcal M$ 
    with a particular event algebra 
    $\mathcal F \subseteq \bigotimes_{m \in C} \mathcal F_m$ corresponding to the set
    of distinctions that frame can see about the labels $m$.
    \[
        \mathscr I_{C, \mathcal F} := \left(\prod_{m \in C} \Omega_m, \mathcal F\right)
    \]
    \noindent
    A information frame could be seen as a perspective on the world. A view of what it
    can in principle see about a system. A probability measure there represents a 
    particular state of the world. For notation convinience we will use 
    $\mathscr I_1 = \mathscr I_{C1, \mathcal F_1}$
\end{definition}

In general, Information Frames are defined on the support of some probability measure,
meaning we get rid of events that a context does not see as possible. \begin{definition}[Structural Contextuality]
    Given a family of contexts $\mathcal C$ of $\mathcal M$ and corresponding 
    information frames $\mathscr I_{C, \mathcal F_C}$ and a global space 
    $\left(\Omega, \mathcal F\right)$. 

    The family is said to be \emph{structuraly non-contextual} if there is a family of 
    corresponding \emph{embeddings} and some sub-$\sigma$-algebra 
    $\mathcal G \subseteq \mathcal F$ such that:
    \[
        \iota_{C \in \mathcal C}: \mathscr I_{C, \mathcal F_C} \to 
        \left(\Omega, \mathcal G\right)
    \]
    Such that for every pair $C,C'$ and every event 
    $E \in \mathcal F_C \cap \mathcal F_{C'}$, that is events on shared labels, we have:
    \[
        \iota_C(E) = \iota_{C'}(E)
    \]
    \noindent
    If no such embedding exists then the family is called \emph{structuraly contextual}.
\end{definition}

The existence of \emph{strong contextuality} as seem in \cite{Abramsky&Brandenburger} 
guarantees structural contextuality also exists, in particular the results in 
\cite{Kochen-Specker} show that, if we try to keep the full $\sigma$-algebra event structure, 
structural contextuality will eventually happen. This shows contextuality is 
essentially inevitable in sufficiently complex logics.
\section{The Relativity Of Information Frames}
\subsection{Interaction}
Now we work on making precise the meaning of the \emph{Relativity of Information Frames}, 
RIF, for short. To do so, the central objects of study will be \emph{Information Frames} as
in defintion 4~.1. Information frames are the only ontic objects in this theory, 
everything works on their interactions.

We now turn our attetion to making the definition of interaction precise. 
The first step is to define the \emph{Joint Frame}. A information frame where 
the interaction takes place. Before this we will note that when we write:
\[
    \iota(\mathcal F_C) := \mathcal G 
\]
That is, there is some sub-$\sigma$-algebra $\mathcal G$ on the global space where that
embedding exits. We also note that there can be many such embeddings. 
These differences are not of particular interest to us, they are related 
by the automorphism symmetry defined in Definition~3.7, so we can treat them as equivalent.
\begin{definition}[Joint Frame]
    Given a family of information frames $\left\{\mathscr I_i\right\}_{i \in I}$ we 
    can construct their joint frame as:
    \[
        \mathcal J_{I} :=\left( \Omega_{\mathcal J_I},\; \mathcal F_{\mathcal J_I}\right) 
        := \left(\prod_{m \in \{\bigcup_{i \in I}C_i \subseteq \mathcal M\}}
            \Omega_{m},\; 
        \bigotimes_{i \in I}\iota_i\left(\mathcal F_i\right)\right)
    \]
    \noindent
    The space that hold all distinctions of all information frames.
\end{definition}
Such joint frame always exists, but it might be contextual. Structural contextuality tells
us directly when it is even possible to make a non-contextual joint frame.

One important thing to note however, is that once we generate such a joint space, the 
embeddings are no longer nescessarly embeddings. That is because in contextual cases,
the event in the intersection $E$ must choose one of the embeddings. This is critical
for our relativity principle.
\subsection{The Structure of the contexts}
To make our realitivity principle precise we need a way to compare the structure of contexts 
in the joint frame.The problem is once we fix a joint frame $\mathcal J_I$ through some 
family of embeddings $\iota_{i \in I}$ we still can't properly compare the contexts as
they do not live in the global space.

A way to do this is to consider the following map
\begin{definition}[Local Projections]
Given a family of embeddings $\iota_i$ that generated a particular frame $\mathcal J_I$.
The local projections $e_i$ are
\[
    e_i := e_{C_i} := \left( \iota_i\circ \pi_{\mathcal J_I \to C_i}\right): 
    \Omega_{\mathcal J_I} \to \Omega_{\mathcal J_I}
\]
\end{definition}
These $e_{i}$ represent the full event structure of the frame $\mathscr I_i$ in the joint
frame.
\subsection{The Symmetry Of Information}
\subsubsection*{Privilege}
We are finally ready to introduce the concept of \emph{The Relativity of Information Frames}.
We begin with the definition of \emph{Privilege} in a \emph{Joint Frame}.
\begin{definition}[Privilege]
    We say a joint frame $\mathcal J_{C_1 \cup C_2}$ privileges $C_1$ over $C_2$ if for
    some shared event $E \in C_1 \cap C_2$ if, for its local projections $e_1,e_2$ we have:
    \[
        e_1(E) = e_1(e_2(E)) \quad \text{ and } \quad e_2(E) \neq e_2(e_1(E))
    \]
\end{definition}
Intuitively, when the joint frame was built, when there is contextuality the joint frame
had to pick one $\iota$ over another for each event they disagreed upon. That is favoring
on that concept over that event.

In particular, its worth noting that the local projections do not commute in the 
contextual case:
\[
    e_1 \circ e_2(E) \neq e_2 \circ e_1(E)
\]
\subsubsection*{The Relativity of Information Frames}
We can now state the relativity of information frames precisely.
\begin{axiom}[The Relativity Of Information Frames]
    After interaction, a physically admissible frame $J_{\text{phys}}$ must not privilege
    any interacting information frame over another on any event.
\end{axiom}

More explicitly for any pair of frames $i,j$ of the joint frame and any shared event $E$
we have:
\[
    e_i(E) = e_i(e_j(E))\quad \text{ and }\quad e_j(E) = e_j(e_i(E))
\]
Equivalently, the event is in the intersection of fixed points:
\[
    E \in \text{Fix}(e_i)\cap \text{Fix}(e_j)
\]
The above condition shows that:
\[
    E \in \text{Fix}(e_i)\cap \text{Fix}(e_j) \quad \forall E \in \mathcal F_{\text{phys}}
    \quad \forall i,j
\]
We define a important object using the definition above, with $\mathcal F_I$ being the $\sigma$-algebra of the full joint frame: 
\begin{definition}[Pointer algebra]
The pointer algebra of a joint frame is given by:
\[
    \mathcal F_{\text{ptr}} := \bigcap_i \text{Fix}(e_i) = \left\{E\in 
    \mathcal F_I :\; e_i(E) = E \;\forall i\right\}
\]
\end{definition}

It is clear from the definition that:
\[
    \mathcal F_{\text{phys}} \subseteq \mathcal F_{\text{ptr}}
\]
In fact, since its obviously true that for all $E \in \mathcal F_{\text{ptr}}$
\[
    e_i(E) = e_i(e_j(E))\quad \text{ and }\quad e_j(E) = e_j(e_i(E))
\]
This intersection is the largest algebra that is physically admissible for the interaction.
\subsection{Symmetry Breaking: Collapse}
Now our study in contextuality already revealed that, the naive embedding joint frame,
that attempts to preserve all distinctions in all information frames, may yield a contextual
joint frame.

\begin{theorem}[Maximality of the pointer algebra]
    The pointer algera $\mathcal F_{\text{ptr}}$ is the largest $\sigma$-algebra that
    is physically admissible under Axiom~1.
\end{theorem}
\begin{proof}
Such joint frame cannot obey Axiom~1. Because structural contextuality shows that for any
joint frame $\mathcal J_I$ that is built from a contextual family there is some event 
$E \in \mathcal F_I$ and some pair $i,j$ for which 
\[
    e_i(E) \neq e_j(E)
\]
Therefore for at least some particular $e_i$ we must have:
\[
    \mathcal F_{\text{ptr}} \subseteq \text{Fix}(e_i)
\]
And for any $\mathcal F_{\text{ptr}}\subsetneq \mathcal G \subseteq \mathcal F_I$
we must have:
\[
    E \in \mathcal G \setminus \mathcal F_{\text{ptr}} \quad \exists k \in I \rightarrow 
    e_k(e_k(E)) \neq e_k(E) 
\]
But by construction of the joint frame, the event $e_k(E)$ must have come from some 
context. So we must have $\iota_j(E) = e_k(E)$. And this means:
\[
 e_j(E) = e_j(e_k(E)) \quad \text{ and } \quad  e_k(e_j(E)) = e_k(e_k(E)) \neq e_k(E)
\]
\end{proof}
And finally we have:
\begin{theorem}[Collapse Theorem]
    Let $\mathcal F_I$ be the contextual joint $\sigma$-algebra of the joint frame generated
    by the interaction of the contextual families $\mathcal F_I$. Let $\mathcal F_{text{ptr}}$
    be its pointer algebra.

    Then under Axiom~1 we have $\mathcal F_{\text{phys}} = \mathcal F_{\text{ptr}}$.
\end{theorem}
\begin{proof}
    For every event $E \not\in \mathcal F_{\text{ptr}}$ by Theorem~5.5 we know it exhibits 
    privilege for some pair of contexts. Therefore such events are physically forbidden
    by RIF. Every event in $\mathcal F_{\text{ptr}}$ is physically admissible. So we have:
    \[
\mathcal F_{\text{phys}} = \mathcal F_{\text{ptr}}
    \]
\end{proof}
\subsection{The Markov View}
A important thing to note is that, once a joint frame has been built, and it has picked
a algebra. The initial embeddings are no longer nescessarely embeddings in that frame.

In fact, for the ones that are not privileged the kernel generated by the original embedding
is no longer injective. It is a strict coarse graining. This will allow us to give precise
numerics and dynamics by implementing information geometry once we are free to assume
states (particular probability distributions). But this is not in the scope of this paper.
\section{Born Rule Martingale}
\section{Arrow of Time}
\section{Locality Graph}
\section{Maximum Speed}
\section{Markovianity}
\section{Wigner's Friend Consistency}
\section{Intrepetation and Comparisons}
\subsection{Spin Example}
\subsection{The Ontology of Information Frames}
\subsection{Comparisons - Collapse Models}
\subsection{Comparisons - Intrepetations}
\section{Discussion}
\subsection{The Hilbert Space Reconstruction}
It is well known that the contextual space, here the joint frame, form a boolean lattice
that recovers most of the properties of a Hilbert Space. 

This view gives us a additional tool, mainly the automorphisms or the idea that, when a 
event must collpase down to a coarse grained version it picks up a phase, representing 
all the distinctions it could have held that map down to the same event. That might allow
us to get the complex field.

The rest of the reconstruction should already have precedent with contextuality frameworks.
\subsection{The Fisher metric view}
By taking the Markov view in section 5.5 we can explore this framework from the point of
view of the fisher metric. When contextuality happens, the ricci curvature of the fisher
metric blows up. That should trigger a back action on the tensor to adjust for the pointer
basis.

Known derivations of the schrodiger equation using the quantum fisher metric can be explained
and made canonical with this framework.

Furthermore, one can define a action principle of interactions. Using information geometry,
we believe that can give a new way to model physical systems.
\section{Conclusion}

\phantomsection
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{9}

\bibitem{Williams1991}
D. Williams,  
\textit{Probability with Martingales},  
Cambridge University Press, 1991.

\bibitem{Pollard2002}
D. Pollard,  
\textit{A User’s Guide to Measure-Theoretic Probability},  
Cambridge University Press, 2002. 
\bibitem{Kallenberg}
O. Kallenberg,  
\textit{Foundations of Modern Probability},  
Springer, 2002.
\bibitem{Csiszar}
I. Csiszár,
\textit{Information-type measures of difference of probability distributions and indirect observation},  
Studia Sci. Math. Hungarica, 1967.

\end{thebibliography}
\end{document}

