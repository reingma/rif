\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb, physics, geometry, mdframed, authblk}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{mathrsfs}
\geometry{margin=1in}

\title{On Measurement: The Relativity of Information Frames}
\author[ ]{Gabriel Masarin}
\affil[ ]{\texttt{g.masarin@proton.me}}
\date{19 November, 2025}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\usepackage{setspace}
\setstretch{1.07}
\clubpenalty=10000
\widowpenalty=10000
\setlength{\belowdisplayskip}{0.8\baselineskip}
\setlength{\abovedisplayskip}{0.8\baselineskip}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newmdenv[
  backgroundcolor=gray!10,
  linecolor=gray!50,
  linewidth=1pt,
  roundcorner=6pt,
  innertopmargin=8pt,
  innerbottommargin=8pt,
]{principlebox}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheoremstyle{axiomstyle}% <name>
  {8pt}% <Space above>
  {8pt}% <Space below>
  {\itshape}% <Body font>
  {}% <Indent amount>
  {\bfseries}% <Theorem head font>
  {.}% <Punctuation after theorem head>
  {0.5em}% <Space after theorem head>
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}% <Theorem head spec>

\theoremstyle{axiomstyle}
\newtheorem{axiom}{Axiom}
\begin{document}
\maketitle

\vspace{0.5em}
\begin{abstract}

\end{abstract}

\section{Introduction}

\section{Motivation - The Relativity of Information Frames}

Consider two physical observers, Alice and Bob, each equipped with a clock and a ruler.
To infer a particle’s momentum, they make two position measurements and record 
the elapsed time.

However,
\begin{itemize}
    \item if they agree on the spatial separation, they must disagree on the elapsed time;
    \item if they agree on the elapsed time, the measured spatial separation must differ.
\end{itemize}

Their interactions with the world differ — and so does what each can resolve as an event.

What Alice calls “particle at position $x$ at time $t$” is determined by her 
interaction channels and detection thresholds.

Thus there is no global, frame-independent $\sigma$-algebra of events.  
Every physical system carries its own information frame: a $\sigma$-algebra 
of distinguishable outcomes accessible through its interactions.

Einstein taught that coordinate descriptions are relative while causal order is invariant.  
We extend this principle.

\begin{principlebox}
\begin{center}
\textbf{Relativity of Information Frames (RIF)}

A physical system does not have a predetermined global set of distinctions. 

Different observers/interactions have different information frames.

Physics cannot depend on which frame you choose.
\end{center}

\end{principlebox}

Measurement is not the revelation of a pre-existing global state; it is the joint refinement
(and, when necessary, coarse-graining) of information frames when systems interact.  
From this symmetry, quantum state update, pointer bases, and even causal 
geometry follow as consequences.

\section{Information Frames and Admissible Interactions}

\subsection{Required definitions}
\subsubsection*{Labels and Contexts}
The first important concept the theory relies upon is that of contextuality. All our
definitions here are translated from the \cite{Abramsky&Brandenburger} contextuality
in sheaf-theory.

First we look at the definition of a measurement label. A measurement label intuitively
represents what one can tell apart, that is what questions a system can ask. Which leads
us to the definition:
\begin{definition}[Measurement labels]
    A measurement label is an abstract symbol $m$ that identifies a physical distinction
    we may attempt to extract from the system. Together with its outcome 
    space $(\Omega_m, \mathcal F_m)$. That is:
    \[
        m \to (\Omega_m, \mathcal F_m)
    \]
    The set of all measurement labels the model considers primitive is called $\mathcal M$.
\end{definition}

The next we talk about a set of questions that make sense together. Certain questions
do not make sense together. 

This is the case of the position and momentum used in the 
motivation. In the shared context of alice and bob, asking what is the position and the
momentum would not make sense. Each sees their own view.

\begin{definition}[Context]
    A context $C \subseteq \mathcal M$ is a finite collection of 
    measurement labels that are jointly meaningful. 
    To each context we associate a measurable space:
    \[
        (\Omega_{C}, \mathcal F_{C}) := \left(\prod_{m \in C}
        \Omega_m \;, \bigotimes_{m \in C} \mathcal F_m \right)
    \]
    \noindent
    Where $\mathcal F_1 \otimes \mathcal F_2 = 
    \sigma\bigl(\left\{F_1 \times F_2: F_1 \in \mathcal F_1, F_2 \in \mathcal F_2\right\}\bigr)$
\end{definition}

Within a context we also define the projections:
\begin{definition}[Canonical Context Projections]
The projections for a context $C$ are defined measurable functions, or random variable:
\[
    \pi_{C \to \{m\}}: \Omega_C \to \Omega_m
\]
With:
\[
    \pi_{C \to \{m\}}\bigl(\omega\bigr) = \omega_m \quad \forall \omega \in C
\]
\noindent
That is, the projections map to the context corresponding to the label $m$ within
the context $C$.
\end{definition}
In general, random variables on a context $C$ should be seen as perspectives. Then the
projection is the \emph{perspective} of $C$ on $m$.

For context projection we will also need a inverse definition. Which we define here.
\begin{definition}[Cylinder Embedding]
    For every outcome $\omega_1 \in \Omega_m$ we embed it in the context space $(\Omega_C,\mathcal F_C)$. This gives the definition:
    \[
        \pi_{C\to\{m\}}^{-1}(A) := \left\{\omega \in \Omega_C:\; x_m \in A\right\}
    \]
    \noindent
    Since $\pi$ is a measurable function by definition $\pi_{C\to\{m\}}^{-1}(A) \in \mathcal F_C$.
\end{definition}

We also need the definition of an \emph{empirical model}. It is given as:
\begin{definition}[Empirical model]
    An \emph{empirical model} is a family $\bigl\{e_C \bigr\}_{ C \in \mathcal M}$ 
    of probability measures on $\bigl(\Omega_C, \mathcal F_C\bigr)$. 
    For all $C,C' \in \mathcal M$ and all $D \in C \cap C'$ we have:
    \[
        \bigl(\pi_{C \to D}\bigr)_* \mu_C \; = \; \bigl(\pi_{C' \to D}\bigr)_* \mu_{C'}
    \]
    That is the pushfoward measures overlap.
\end{definition}

Finally we have the definition of contextuality. We modify it slightly for our needs, first
we consider a empirical family defined on the context $C_1$, 
$\bigl\{e_{E} \bigr\}_{ E \in C_1}$. 

\begin{definition}[Contextuality]
    The family $\bigl\{e_{E} \bigr\}_{ E \in C_1}$ is called contextual in $C \supseteq C_1$
    if no probability measure $\mu$ on $C$ exists satisfying:
    \[
        \bigl(\pi_{C \to E}\bigr)_* \mu \; = \mu_{E} \quad \forall E \in C_1
    \]
    \noindent
    They are called non-contextual, if such probability measure exists.
\end{definition}
The idea, is that the empirical model represents certain observables. If they do not have
a joint, consistent way to represent all of them in a that mesurable space. 

These are the pieces from sheaf-theory contextuality besides empirical models, which 
will be introduced later, which we require.
\subsubsection*{Markov Kernels}
We will also need the definition of \emph{Markov Kernels}, which allows us to define how
different measurable spaces interact.
\begin{definition}[Markov Kernels]
    Let $(\Omega_1, \mathcal F_1)$ and $(\Omega_2, \mathcal F_2)$ be two measurable 
    spaces. A \emph{Markov Kernel} is a function:
    \[
        K: \Omega_1 \cross \mathcal F_2 \to [0,1]
    \]
    \noindent
    Where we have:
    \begin{itemize}
        \item For every fixed $\omega \in \Omega_1$ 
            \[
                K(\omega,.) \text{ is a probability measure in } \mathcal F_2
            \]
        \item For every fixed $A \in \mathcal F_2$
            \[
                K(., A) \text{ is a measurable function } \Omega_1 \to \Omega_2
            \]
    \end{itemize}
    \noindent
    For convinience we write $K: \Omega_1 \to \Omega_2$.
\end{definition}
One important use of Markov Kernels we will need is the pushfoward measure:
\begin{definition}[Markov Pushfoward Measure]
    Given the measurable spaces and Kernel on the definition 3~4. Let $\mu$ be a probability 
    measure on $\Omega_1$. The \emph{pushfoward measure} given by the Kernel
    \[
        (\mu K) (A) := \int_{\Omega_1} K(\omega, A) \,\mu (dx), \qquad A \in \mathcal F_2
    \]
    \noindent
    And it is a probability measure on $\mathcal F_2$.
\end{definition}

The other piece that will be important for this theory is:
\begin{theorem}[DPI for KL-Divergence in Markov Kernels]
    Every Markov Kernel safisfies the \emph{data processing inequality} for KL-Divergence.
    Let $\mu$, $\nu$ be probability measures on $(\Omega_1, \mathcal F_1)$ and 
    $K: \Omega_1 \to \Omega_2$ be a Markov Kernel. Then:
    \[
        D_{KL}(\mu K || \nu K) \leq D_{KL}(\mu || \nu)
    \]
    \noindent
    Meaning we only lose or retain KL-distinguishbility by applying Markov Kernels.
\end{theorem}
\begin{proof}
This theorem is prove in \cite{}.
\end{proof}



\subsection{Information Frames}
Now a information frame is analogous to the empirical model.
\begin{definition}[Information Frame]
    Given a context $C$ an information frame on it is:
    \[
        \mathscr I_C = \bigl(\Omega_C, \mathcal F \bigr)
    \]
\noindent
With $\mathcal F$ some sub-sigma algebra of $\mathcal F_C$.
\end{definition}
Intuitively a information frame in RIF is all perspective of a system. 
More generally it can be a system, an apparatus, an observer. It is the only ontic 
object in RIF.
\subsection{Admissible Maps}
\begin{definition}[Admissible Map]

A Markov Kernel is a Admissible Map acts on \emph{information frames} 
$K: \mathscr I_{C_1} \to \mathscr I_{C_2}$ and stastifies:
    \begin{enumerate}
        \item \textbf{Measurement-label preservation.}
            For every shared measurement label $m \in C_1 \cap C_2$ and every $A \in 
            \mathcal F_m$, that is every measurable set in $\mathscr I_{\{m\}}$.
            \[
                K \bigl(\omega,\;\pi_{C_2 \to \{m\}}^{-1}\bigl(A\bigr)\;\bigr) =
                \mathbf 1_{\pi_{C_1 \to \{m\}}^{-1}(A)}\bigl(\omega\bigr) \qquad 
                \omega \in \Omega_{C_1}
            \]
        \item \textbf{Statisfy DPI for KL-Divergence.} Let $\mu$, $\nu$ be probability measures on the frame we must have:
    \[
        D_{KL}(\mu K || \nu K) \leq D_{KL}(\mu || \nu)
    \]
    \end{enumerate}
\end{definition}
The admissibility maps are key to the RIF framewok. So we need to establish some of their
characteristics.

The identity kernels for $\mathscr I_C$ are admissible. 
\begin{lemma}
    The following map: 
    \[
    \mathrm{id}_C(x, A) := \mathbf 1_A(x)
    \]
    Is admissible $\mathrm{id}_C: \mathscr I_C \to \mathscr I_C$.
\end{lemma}

A few properties of admissible maps:
\begin{itemize}
    \item \textbf{Not closed under composition.} We note that admissible maps are not closed under composition in general. That is because
if two contexts do not share a label, then they cannot compose. This will be a key feature
later.
\item \textbf{Preserve shared labels.} Due to property 1, admissible maps must behave 
    like the identity channel on shared degrees of freedom. That is, they are not 
    allowed to add noise, delete information, merge or refine outcomes.

    In a way they preserve the perspective of the source system.
\item \textbf{Have DPI for KL-divergence.} Since Markov Kernels obey DPI in that sense, 
    so must admissible maps.
\item \textbf{Preserve Contextuality Marginals} Since they preserve shared labels, the 
    contextuality structure of the shared labels is preserved.
\end{itemize}

The main idea is that admissible maps preserve perspectives and don't create information. 

As we have seen with the identity map, such maps exists. But for clarity we give a 
non-trivial example:
\subsection{An example of an Admissible Map}
\section{The Joint Frames}
We now define a new concept, that of \emph{joint frames}. Given two frames $\mathscr I_{C_1}$
and $\mathscr I_{C_2}$, we can always build a frame that contains both frames information:
\[
    \mathscr I_{C_1 \cup C_2} := \bigl(\Omega_{C_1} \times \Omega_{C_2}, 
    \mathcal F_{C_1} \otimes \mathcal F_{C_2} \bigr)
\]
That is, this frame preserves all distinctions (their sigma algebras) that both frames
make. 

But in RIF we make a additional restriction. If those frames hold states, we only
allow joint frames that are reacheable by admissible maps.

Since the information frames are ontic, when two frames interact, we must preserve the
perspectives of each frame, \emph{nature does not privilege one perspective over another}.

For that, let us define the $\sigma$-algebra generated by a admissible map. We have:
\begin{definition}[Effective $\sigma$-algebra of K]
    For $K: \mathscr I_1 \to \mathscr I_2$. We define the effective sigma algebra of $K$, 
    denoted $\sigma\bigl( K \bigr)$ as:
    \[
        \sigma(K):= \sigma\bigl\{K(.,A) : A \in \mathcal F_2 \bigr\}
    \]
    \noindent
    That is, the distinctions that survive through $K$.
\end{definition}

Naturally, that $\sigma$-algebra is a sub $\sigma$-algebra of the frame $\mathscr I_2$.

So we define the class of candidate frames:
\begin{definition}[Candidate Joint Frames]
    
\end{definition}


\phantomsection
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{9}

\bibitem{Williams1991}
D. Williams,  
\textit{Probability with Martingales},  
Cambridge University Press, 1991.

\bibitem{WisemanMilburn2009}
H.~M. Wiseman and G.~J. Milburn,  
\textit{Quantum Measurement and Control},  
Cambridge University Press, 2009.

\bibitem{Pollard2002}
D. Pollard,  
\textit{A User’s Guide to Measure-Theoretic Probability},  
Cambridge University Press, 2002. 
\bibitem{Kallenberg}
O. Kallenberg,  
\textit{Foundations of Modern Probability},  
Springer, 2002.
\bibitem{Csiszar}
I. Csiszár,
\textit{Information-type measures of difference of probability distributions and indirect observation},  
Studia Sci. Math. Hungarica, 1967.

\end{thebibliography}
\end{document}

